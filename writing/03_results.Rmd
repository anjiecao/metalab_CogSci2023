
## Functional form of developmental curves


```{r eval=FALSE, include=FALSE}
# code chunks to generate the table which needs to be hand adjusted

age_df <- readRDS(here("cached_data/age_models_df.Rds"))
summary_df <- readRDS(here("cached_data/summary_d.Rds"))

summary_d_print <- summary_df %>% 
  mutate(es_print = 
           paste0(round(es, 2), " [", round(es_lb, 2), ",", round(es_ub, 2), "]"), 
           #round(es, 2),
         
         mean_age_print = round(mean_age, 2), 
         n_kids_print = round(n_kids, 0),
         age_range_print = paste0("[", round(min_age, 2), ",", round(max_age, 2), "]")
           ) %>% 
  select(ds_clean, n_studies, n_effects, n_kids_print, mean_age_print, age_range_print, es_print )


age_df_wide <- age_df %>% 
  filter(ic == "AICc") %>% 
  rename(Dataset = dataset) %>% 
  select(REML, model_spec_clean, Dataset) %>% 
  pivot_wider(names_from = model_spec_clean, 
              values_from = REML)

age_df_wide$min <- apply(age_df_wide[c('Linear','Log', 'Quadratic', 'Const')], 1, min)



age_df_wide %>% 
  left_join(summary_d_print %>% select(ds_clean, n_effects, n_kids_print, es_print) %>% rename(Dataset = ds_clean), by = "Dataset") %>% 
  select(Dataset, n_effects, n_kids_print, es_print, Linear, Log, Quadratic, Const) %>% 
  rename(`N ES` = n_effects, 
         `N Participants` = n_kids_print, 
         `ES` = es_print,
         Constant = Const) %>% 
  mutate(across(where(is.numeric), round, 2)) %>% 
  kable()

```





\begin{table*}[ht]
\begin{tabular}{l|r|r|r|r|r|r|r}

\hline
\textbf{Dataset} & N ES & N Participants & ES estimates & Linear & Log & Quad. & Const.\\
\hline
Label advantage in concept learning & 100 & 1644 & 0.36 [0.23,0.48] & 169 & \textbf{169} & 170 & 171\\
Vowel discrimination (native) & 143 & 2418 & 0.59 [0.43,0.75] & 256 & 256 & 258 & \textbf{255}\\
Vowel discrimination (non-native) & 49 & 600 & 0.65 [0.2,1.1] & 73 & 73 & 73 & \textbf{72}\\
Statistical word segmentation & 103 & 804 & -0.08 [-0.18,0.02] & 129 & 129 & 129 & \textbf{128}\\
Online word recognition & 14 & 330 & 1.37 [0.78,1.96] & \textbf{47} & 47 & 47 & 49\\
Mutual exclusivity & 131 & 2222 & 1.27 [0.99,1.56] & 422 & \textbf{416*} & 433 & 453\\
Sound symbolism & 44 & 425 & 0.16 [-0.01,0.33] & 58 & \textbf{58} & 59 & 61\\
Categorization bias & 80 & 382 & 0.25 [-0.54,1.05] & 300 & \textbf{300} & 300 & 301\\
Familiar word recognition & 34 & 586 & 0.54 [0.38,0.69] & 27 & 28 & \textbf{27} & 29\\
Abstract rule learning & 95 & 1123 & 0.22 [0.07,0.37] & 141 & 141 & \textbf{140} & 141\\
Switch task & 143 & 2764 & -0.16 [-0.25,-0.06] & 205 & 205 & 205 & \textbf{204}\\
Mispronunciation sensitivity & 249 & 2122 & 0.45 [0.24,0.66] & 620 & 628 & \textbf{614*} & 644\\
Prosocial agents & 61 & 1244 & 0.4 [0.29,0.52] & 82 & 82 & 82 & \textbf{80}\\
Simple arithmetic competences & 14 & 369 & 0.25 [0.04,0.46] & 23 & 23 & 23 & \textbf{16}\\
Symbolic play & 196 & 7148 & 0.63 [0.53,0.72] & 234 & 234 & 234 & \textbf{234}\\
Natural speech preference & 55 & 786 & 0.44 [0.23,0.65] & 111 & 112 & \textbf{111} & 112\\
Cross-situational word learning & 48 & 2241 & 0.67 [0.5,0.84] & 80* & 82* & \textbf{80*} & 84\\
Language discrimination and preference & 153 & 2060 & -0.13 [-0.26,0] & 265 & 266 & \textbf{263} & 265\\
Syntactic bootstrapping & 60 & 832 & 0.24 [0.03,0.44] & 107 & \textbf{107} & 108 & 107\\
Statistical sound category learning & 11 & 350 & 0.56 [0.19,0.93]& 33* & 35 & 33* & \textbf{30*}\\
Gaze following & 81 & 1407 & 0.81 [0.61,1.01] & 152* & 160 & \textbf{149*} & 193\\
Word Segmentation & 315 & 2910 & 0.2 [0.14,0.26] & 329 & 329 & 329 & \textbf{328}\\
Infant directed speech preference & 83 & 985 & 0.47 [0.28,0.65] & 70 & 71 & 70 & \textbf{69}\\
\hline
\end{tabular}
\caption{\label{demo-table}This table summarize the number of effect sizes \(ES\) and the number of participants included in each meta-analysis dataset. The ES estimates represent the aggregated effect sizes and their 95\% confidence intervals from each dataset. The last four columns include the values of corrected Akaike Information Criterion \(AICc\) for the age model with different functional forms: Linear, Logarithmic, Quadratic and the Constant. The values were rounded to integers and the bold values are the smallest values among the four functional forms before rounding. Asterisks mark the models that are significantly better than the other models in the same research area.}
\end{table*}

```{r}



age_df_wide <- age_df %>% 
  filter(ic == "AICc") %>% 
  rename(Dataset = dataset) %>% 
  select(REML, model_spec_clean, Dataset) %>% 
  pivot_wider(names_from = model_spec_clean, 
              values_from = REML)
age_df_wide$min <- apply(age_df_wide[c('Linear','Log', 'Quadratic', 'Const')], 1, min)


model_comparison_df <- age_df_wide %>% 
  mutate(
    lin_diff = Linear - min, 
    log_diff = Log - min, 
    qua_diff = Quadratic - min, 
    const_diff = Const - min
  ) %>% 
  select(Dataset, contains("diff")) %>% 
  pivot_longer(cols = contains("diff"), 
               names_to = "diff_type", 
               values_to = "diff_value") %>% 
  mutate(diff_category = case_when(
    diff_value < 4 ~ "no_diff", 
    diff_value >= 4 & diff_value < 7 ~ "low_win", 
    diff_value > 10 ~ "refute_high"
  )) %>% 
  #filter(diff_category != "no_diff") %>% 
  select(-diff_category) %>% 
  pivot_wider(names_from = diff_type, values_from = diff_value) %>% 
  mutate_if(is.numeric, round, 2)
```



Our first research question was about the functional form of the developmental trajectories we observed. We considered four specific forms: linear, logarithmic, quadratic, and constant, each considered as an age-related fixed effect.  We evaluated the models based on the corrected AICc (Table 1).


When using AICc in model selection, the value needs to be contextualized in relation to the lowest AICc. Under the conventional interpretation, $\Delta_{i}$ ($AIC_i - AIC_{min}$, where $AIC_i$ is the model being evaluated, and $AIC_{min}$ is the lowest AIC among the set of models evaluated) less than 4 suggests minimal evidence against the model with higher AICc; $\Delta_{i}$ above 4 suggests substantial support for the model with lower AICc [@burnham2004multimodel]. With this interpretation framework, the functional forms in most domains can not be meaningfully distinguished, with exceptions in 6 domains. In Mutual Exclusivity, there is a strong preference for the logarithmic model ($\Delta_{Linear}$ = `r filter(model_comparison_df, Dataset == "Mutual exclusivity")$lin_diff` ; $\Delta_{Quad.}$ = `r filter(model_comparison_df, Dataset == "Mutual exclusivity")$qua_diff`; $\Delta_{Const.}$ = `r filter(model_comparison_df, Dataset == "Mutual exclusivity")$const_diff`). We also found a strong preference for the quadratic model in Mispronunciation sensitivity ($\Delta_{Linear}$ = `r filter(model_comparison_df, Dataset == "Mispronunciation sensitivity")$lin_diff`; $\Delta_{Const.}$ = `r filter(model_comparison_df, Dataset == "Mispronunciation sensitivity")$const_diff`; $\Delta_{Log}$ = `r filter(model_comparison_df, Dataset =="Mispronunciation sensitivity")$log_diff`) and a strong preference for the constant model in Simple arithmetic competence ($\Delta_{Linear}$ = `r filter(model_comparison_df, Dataset == "Simple arithmetic competences")$lin_diff`; $\Delta_{Quad.}$ = `r filter(model_comparison_df, Dataset == "Simple arithmetic competences")$qua_diff`; $\Delta_{Log}$ = `r filter(model_comparison_df, Dataset =="Simple arithmetic competences")$log_diff`). The comparison is less clear-cut in Gaze following, where there is support for the Quadratic model against the Constant model and the Logarithmic model ( $\Delta_{Const.}$ = `r filter(model_comparison_df, Dataset == "Gaze following (combined)")$const_diff`; $\Delta_{Log}$ = `r filter(model_comparison_df, Dataset =="Gaze following (combined)")$log_diff`), but the Linear model is comparable with the Quadaratic model ($\Delta_{Linear}$ = `r filter(model_comparison_df, Dataset =="Gaze following (combined)")$lin_diff`). Finally, in Statistical sound category learning and Cross situational word learning, we only found evidence against the logarithmic model ($\Delta_{Log}$ = `r filter(model_comparison_df, Dataset == "Statistical sound category learning (habituation)")$log_diff`) and the constant model ($\Delta_{Log}$ = `r filter(model_comparison_df, Dataset == "Cross-situational word learning")$const_diff`), respectively. 


## Methodological Moderators 

In this section, we considered methodological moderators shared by multiple datasets. Given the limited number of studies conducted with neuroimaging methods, we focused our analyses on studies conducted with behavioral methods. Therefore, we excluded studies that were conducted with either fNIRS or EEG.  Moreover, to minimize age-related heterogeneity, we only included studies with participants’ mean age below 36 months. All analyses were conducted on the subset of research domains with multiple levels for the moderator of interests. 

Figure 1 provides a summary of the estimates for moderators. 




```{r}
# baseline is looking 
bmeasure_df <- best_fit_bmeasure_model_df %>% 
  filter(grepl("behavioral_measure", term)) %>% 
  mutate(term_print = case_when(
    grepl("other", term) ~ "Behavioral Measure - Other", 
    TRUE ~ "Behavioral Measure - Sucking"
  )) %>% 
  select(dataset, term_print, statistic, p.value, estimate, std.error) 

bmeasure_int_df <- best_fit_bmeasure_interaction_df %>% 
   filter(grepl("behavioral_measure", term) & grepl("age", term)) %>% 
  mutate(term_print = "bmeasure_age_interaction") %>% 
  select(dataset, term_print, statistic, p.value, estimate, std.error) 


# baseline is conditionining 
ep_df <- best_fit_exposure_phase_df %>% 
  filter(grepl("exposure_phase", term)) %>% 
  mutate(term_print = case_when(
    grepl("habituation", term) ~ "Exposure phase - Habituation", 
    grepl("test_only", term) ~ "Exposure phase - Familiarization", 
    grepl("familiarization", term) ~ "Exposure phase - Test only"
  )) %>% 
  select(dataset, term_print, statistic,p.value,  estimate, std.error) 

rv_df <- best_fit_real_visual_df %>% 
  filter(grepl("visual", term)) %>% 
  mutate(term_print = "Visual stimulus type - Natural") %>% 
  select(dataset, term_print,statistic, p.value,  estimate, std.error) 


av_df <- best_fit_real_aud_df %>% 
  filter(grepl("aud", term)) %>% 
  mutate(term_print = "Auditory stimulus type - Natural") %>% 
  select(dataset, term_print, statistic, p.value, estimate, std.error) 

ma_df <- best_fit_major_author_df %>% 
  filter(grepl("author", term)) %>% 
  mutate(term_print = "By Major Author") %>% 
  select(dataset, major_author, term_print, statistic, p.value, estimate, std.error)
```


```{r}
all_mod_df <- bind_rows(bmeasure_df, bmeasure_int_df, ep_df, rv_df, av_df, ma_df) %>% 
  mutate(lb = estimate - std.error  * 1.96, 
         ub = estimate + std.error * 1.96) %>% 
  mutate(estimate_print = paste0(round(estimate, 2), " [", round(lb, 2), ", ", round(ub, 2), "]"), 
         z_print = round(statistic, 2), 
         p_print = case_when(
           p.value >= 0.01 ~ paste0("= ", round(p.value, 2)), 
           TRUE ~ "< 0.01"
         )) %>% 
  separate(term_print, into = c("type", "group"), sep = "-")

```


### Behavioral Measures 


```{r}
bm_models <- all_mod_df %>% filter(type == "Behavioral Measure ")
bm_int <- all_mod_df %>% filter(type == "bmeasure_age_interaction")

int_improved_dataset <- best_fit_bmeasure_model_df %>% 
  distinct(dataset, REML) %>% 
  rename(reg_REML = REML) %>% 
  left_join(best_fit_bmeasure_interaction_df %>% distinct(dataset, REML) %>% rename(int_REML = REML), by = "dataset") %>% 
  filter(int_REML < reg_REML)


```

Meta-analyses have very heterogeneous moderators coded, but many included coding of which behavioral response measure was used in the original study: looking-based behaviors (e.g., looking time or other eye-tracking measures), sucking (as in the high amplitude sucking procedure), and manual behaviors (e.g., pointing, exploration). We thus added behavioral measure as an additional fixed effect to the age model with the best-fitting functional form from the previous analysis. 

In general, nearly all effects were weakly positive such that sucking and manual response modes yielded slightly larger effect sizes, though these effects were not always significant. Behavioral measure was a significant predictor of effect sizes in only two domains, Vowel Discrimination (Native) and Sound Symbolism. In Vowel Discrimination (native), studies with Manual or Sucking behavioral measure has larger effect sizes than studies using looking as the behavioral measure (Manual: $\beta$ = `r filter(bm_models, dataset == "Vowel discrimination (native)", group == " Other")$estimate_print`, *z* = `r filter(bm_models, dataset == "Vowel discrimination (native)", group == " Other")$z_print`, *p* `r filter(bm_models, dataset == "Vowel discrimination (native)", group == " Other")$p_print` ; Sucking: $\beta$ = `r filter(bm_models, dataset == "Vowel discrimination (native)", group == " Sucking")$estimate_print`, *z* = `r filter(bm_models, dataset == "Vowel discrimination (native)", group == " Sucking")$z_print`, *p* `r filter(bm_models, dataset == "Vowel discrimination (native)", group == " Sucking")$p_print`). Similarly, in Sound Symbolism, studies with manual behavioral measures also yield larger effect sizes than looking studies ($\beta$ = `r filter(bm_models, dataset == "Sound symbolism")$estimate_print`, *z* = `r filter(bm_models, dataset == "Sound symbolism")$z_print`, *p* `r filter(bm_models, dataset == "Sound symbolism")$p_print`). 

We also explored whether there would be an interaction between the research method and participants’ age. The inclusion of interaction terms did not meaningfully improve the AICc of any of the main model (All $\Delta_{interaction}$s < 2). The current datasets can not distinguish between the interaction effect and the main effect. 


### Exposure Phase 

```{r}
ep_models <- all_mod_df %>% filter(type == "Exposure phase ")
```

Exposure phase refers to the type of exposure infants have during the experiments prior to the test events. There are typically four types of exposure phase: 1) an infant would be conditioned to show an orienting behavior (conditioning); 2) an infant would be exposed to a stimulus for a constant amount of time (familiarization); 3) an infant would be shown some stimulus repeatedly until the magnitude of response drops below a threshold (habituation); 4) no exposure to the stimulus at all; the infant would be tested immediately (test-only). We coded these four types of exposure phases as four levels in the moderator exposure phase. 

Exposure phase is a significant predictor of effect sizes in four domains. In Language Discrimination and Preference,  habituation studies yield smaller effect sizes than conditioning studies ($\beta$ = `r filter(ep_models, dataset =="Language discrimination and preference", group == " Habituation")$estimate_print`, *z* = `r filter(ep_models, dataset =="Language discrimination and preference", group == " Habituation")$z_print`, *p* `r filter(ep_models, dataset =="Language discrimination and preference", group == " Habituation")$p_print`). A similar trend is found in Vowel discrimination (native), where both habituation studies and familiarization studies have smaller effect sizes than conditioning studies (Habituation: $\beta$ = `r filter(ep_models, dataset =="Vowel discrimination (native)", group == " Habituation")$estimate_print`, *z* = `r filter(ep_models, dataset =="Vowel discrimination (native)", group == " Habituation")$z_print`, *p* `r filter(ep_models, dataset =="Vowel discrimination (native)", group == " Habituation")$p_print`; Familiarization: $\beta$ = `r filter(ep_models, dataset =="Vowel discrimination (native)", group == " Familiarization")$estimate_print`, *z* = `r filter(ep_models, dataset =="Vowel discrimination (native)", group == " Familiarization")$z_print`, *p* `r filter(ep_models, dataset =="Vowel discrimination (native)", group == " Familiarization")$p_print`). 

Interestingly, the results of the comparison between familiarization studies and conditioning studies are mixed. In the Vowel discrimination (non-native) dataset, both familiarization and test-only studies produce smaller effect sizes than the conditioning studies (Familiarization: $\beta$ = `r filter(ep_models, dataset =="Vowel discrimination (non-native)", group == " Familiarization")$estimate_print`, *z* = `r filter(ep_models, dataset =="Vowel discrimination (non-native)", group == " Familiarization")$z_print`, *p* `r filter(ep_models, dataset =="Vowel discrimination (non-native)", group == " Familiarization")$p_print`; Test-only:  $\beta$ = `r filter(ep_models, dataset =="Vowel discrimination (non-native)", group == " Test only")$estimate_print`, *z* = `r filter(ep_models, dataset =="Vowel discrimination (non-native)", group == " Test only")$z_print`, *p* `r filter(ep_models, dataset =="Vowel discrimination (non-native)", group == " Test only")$p_print`). However, the opposite trend is found in Natural Speech Preference, with familiarization studies producing larger effect sizes than the conditioning studies ($\beta$ = `r filter(ep_models, dataset =="Natural speech preference", group == " Test only")$estimate_print`, *z* = `r filter(ep_models, dataset =="Natural speech preference", group == " Test only")$z_print`, *p* `r filter(ep_models, dataset =="Natural speech preference", group == " Test only")$p_print`).

### Stimuli Naturalness 

```{r}
vis_mod <- all_mod_df %>% filter(type == "Visual stimulus type ")
aud_mod <-  all_mod_df %>% filter(type == "Auditory stimulus type ")
```

Next, we considered the effect of stimuli type. We focused on one key dimension: naturalness. For primarily visual stimuli, we considered “natural” to mean stimuli that use real-world objects (e.g. puppets, blocks). We compared these natural stimuli with representation-type stimuli, such as pictures, videos, or drawings. In primarily auditory stimuli, we compared recorded natural speech with synthesized stimuli. 

Natural stimuli across modalities has advantages over artificial stimuli. We found that naturalness is a significant predictor for Label advantage in concept learning, with natural stimuli yielding larger effect sizes than representation-type stimuli ($\beta$ = `r filter(vis_mod, dataset =="Label advantage in concept learning")$estimate_print`, *z* = `r filter(vis_mod, dataset =="Label advantage in concept learning")$z_print`, *p* `r filter(vis_mod, dataset =="Label advantage in concept learning")$p_print`). Similarly, in both Statistical word segmentation and Abstract rule learning, we found a natural speech advantage (Statistical word segmentation: $\beta$ = `r filter(aud_mod, dataset =="Statistical word segmentation")$estimate_print`, *z* = `r filter(aud_mod, dataset =="Statistical word segmentation")$z_print`, *p* `r filter(aud_mod, dataset =="Statistical word segmentation")$p_print`; Abstract rule learning: $\beta$ = `r filter(aud_mod, dataset =="Abstract rule learning")$estimate_print`, *z* = `r filter(aud_mod, dataset =="Abstract rule learning")$z_print`, *p* `r filter(aud_mod, dataset =="Abstract rule learning")$p_print`). 


```{r mod_plot, fig.env = "figure*", fig.pos = "t!", fig.width=7, fig.height=5, fig.align = "center", set.cap.width=T, num.cols.cap=2, fig.cap = "Each panel shows the moderator estimates. Each dot represents the estimate of the particular moderator level compared to the baseline. For behavioral measure, the baseline level is looking. Red dots indicate the estimate for studies using sucking measure, and the teal dots indicate the estimates for studies using manual measure. For exposure phase, the baseline level is conditioning. Red, blue, and green represent the estimates for studies using familiarization, test-only, and habituation in exposure phase, respectively. For stimuli naturalness, the dots represent the estimate for studies using natural stimuli (e.g. real-world objects; natural speech) compared to studies using artificial stimuli (e.g. pictures, synthetic speech)."}

plot_moderators(all_mod_df)
```

### Major author 

@margoni2018infants found evidence for an author-based bias in Prosocial Agents, where results produced by certain authors were consistently larger than others. We evaluated how prevalent this phenomenon was in the literature by coding a “major author” moderator. Authors are considered to be a “major author” if they are listed as authors in more than 15% of the papers in the research area. When multiple major authors co-authored the same set of publications, we considered one author from that author group. When multiple authors are considered as major authors but are associated with different publications, we selected the ones with the most publications in the research area 

We found evidence for a major author effect in 7 datasets, where studies produced by the major author were larger than the rest of the papers. In 3 datasets, however, we also found the opposite patterns, where certain authors produced on average smaller effect sizes than the rest of the literature. 

```{r image, fig.env = "figure", fig.pos = "H", fig.align='center', fig.width=3, fig.height=3, set.cap.width=T, num.cols.cap=1, fig.cap = "Each dot represents the estimate for studies produced by major author in the particular research area, compared to other studies in the same research area."}
plot_major_author(all_mod_df)
```

## Synthesis 

Finally, we synthesized all 23 datasets by grouping them based on the type of theoretical constructs they represented: Cognitive, Communication, Sounds, and Words. We integrated the predictions from the best-fitting age-based models in Figure 2, showing predictions across the range of measured phenomena. We found a striking range of functional forms in the developmental trajectories across all types of theoretical constructs. In particular, the magnitudes of some phenomena – online word recognition, gaze following, and mutual exclusivity, for example – increased substantially over development. In contrast, others – sound symbolism, categorization bias, and others – stayed constant at a measurable level without showing developmental increases. We considered several explanations for these differences: one is that these meta-analyses might correspond to relatively more experience-independent biases. On the other hand, we cannot rule out cross-experiment confounding wherein experimenters test progressively harder stimuli with development, thus counteracting any developmental gains that might otherwise be measured.  


