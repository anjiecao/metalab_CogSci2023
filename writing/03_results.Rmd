
## Functional form of developmental curves


```{r eval=FALSE, include=FALSE}
# code chunks to generate the table which needs to be hand adjusted

age_df <- readRDS(here("cached_data/age_models_df.Rds"))
summary_df <- readRDS(here("cached_data/summary_d.Rds"))

summary_d_print <- summary_df %>% 
  mutate(es_print = 
           paste0(round(es, 2), " [", round(es_lb, 2), ",", round(es_ub, 2), "]"), 
           #round(es, 2),
         
         mean_age_print = round(mean_age, 2), 
         n_kids_print = round(n_kids, 0),
         age_range_print = paste0("[", round(min_age, 2), ",", round(max_age, 2), "]")
           ) %>% 
  select(ds_clean, n_studies, n_effects, n_kids_print, mean_age_print, age_range_print, es_print )


age_df_wide <- age_df %>% 
  filter(ic == "AICc") %>% 
  rename(Dataset = dataset) %>% 
  select(REML, model_spec_clean, Dataset) %>% 
  pivot_wider(names_from = model_spec_clean, 
              values_from = REML)

age_df_wide$min <- apply(age_df_wide[c('Linear','Log', 'Quadratic', 'Const')], 1, min)

# get the delta value 
age_df_wide <- age_df_wide %>% 
  mutate(d_linear = Linear - min, 
         d_log = Log - min, 
         d_quad = Quadratic - min, 
         d_const= Const - min) %>% 
  select(Dataset, d_linear, d_log, d_quad, d_const) %>% 
  rename(Linear = d_linear, 
         Log = d_log, 
         Quadratic = d_quad, 
         Const = d_const) %>% 
   mutate(across(where(is.numeric), round, 2)) 



age_df_wide %>% 
  left_join(summary_d_print %>% select(ds_clean, n_effects, n_kids_print, es_print) %>% rename(Dataset = ds_clean), by = "Dataset") %>% 
  select(Dataset, n_effects, n_kids_print, es_print,Const,  Linear, Log, Quadratic) %>% 
  rename(`N ES` = n_effects, 
         `N Participants` = n_kids_print, 
         `ES` = es_print,
         Constant = Const) %>% 
  mutate(across(where(is.numeric), round, 1)) %>% 
  kable()

```



\begin{table*}[ht]
\begin{tabular}{l|r|r|l|r|r|r|r}
\hline
\textbf{Dataset} & N ES & N Participants & ES & Const. & Linear & Log & Quad.\\
\hline
Statistical sound category learning  & 11 & 350 & 0.56 [0.19,0.93] & \textbf{0.0*} & 3.0* & 4.1 & 2.5*\\
Vowel discrimination (native) & 143 & 2418 & 0.59 [0.43,0.75] & \textbf{0.0} & 1.3 & 1.0 & 1.6\\
Vowel discrimination (non-native) & 49 & 600 & 0.65 [0.2,1.1] & \textbf{0.0} & 1.6 & 1.7 & 1.5\\
Statistical word segmentation & 103 & 804 & -0.08 [-0.18,0.02] & \textbf{0.0} & 1.3 & 1.5 & 1.1\\
Switch task & 143 & 2764 & -0.16 [-0.25,-0.06] & \textbf{0.0} & 1.1 & 1.1 & 1.1\\
Prosocial agents & 61 & 1244 & 0.4 [0.29,0.52] & \textbf{0.0} & 2.1 & 1.9 & 2.1\\
Simple arithmetic competences & 14 & 369 & 0.25 [0.04,0.46] & \textbf{0.0} & 6.7 & 6.7 & 6.6\\
Symbolic play & 196 & 7148 & 0.63 [0.53,0.72] & \textbf{0.0} & 0.6 & 0.5 & 0.6\\
Word Segmentation  & 315 & 2910 & 0.2 [0.14,0.26] & \textbf{0.0} & 1.3 & 1.0 & 1.6\\
Infant directed speech preference & 83 & 985 & 0.47 [0.28,0.65] & \textbf{0.0} & 1.0 & 1.8 & 0.9\\
Online word recognition & 14 & 330 & 1.37 [0.78,1.96] & 2.2 & \textbf{0.0} & 0.2 & 0.1\\
Mutual exclusivity & 131 & 2222 & 1.27 [0.99,1.56] & 37.2 & 5.8 & \textbf{0.0*} & 16.9\\
Label advantage in concept learning & 100 & 1644 & 0.36 [0.23,0.48] & 2.4 & 0.9 & \textbf{0.0} & 1.6\\
Sound symbolism & 44 & 425 & 0.16 [-0.01,0.33] & 2.9 & 0.0 & \textbf{0.0} & 0.7\\
Categorization bias & 80 & 382 & 0.25 [-0.54,1.05] & 0.9 & 0.3 & \textbf{0.0} & 0.4\\
Syntactic bootstrapping & 60 & 832 & 0.24 [0.03,0.44] & 0.5 & 0.3 & \textbf{0.0} & 0.6\\
Mispronunciation sensitivity & 249 & 2122 & 0.45 [0.24,0.66] & 30.7 & 6.4 & 14.5 & \textbf{0.0*}\\
Cross-situational word learning & 48 & 2241 & 0.67 [0.5,0.84] & 4.0 & 0.1* & 1.9* & \textbf{0.0*}\\
Gaze following  & 81 & 1407 & 0.81 [0.61,1.01] & 43.7 & 2.1* & 10.4 & \textbf{0.0*}\\
Familiar word recognition & 34 & 586 & 0.54 [0.38,0.69] & 1.7 & 0.3 & 1.1 & \textbf{0.0}\\
Abstract rule learning & 95 & 1123 & 0.22 [0.07,0.37] & 0.4 & 0.3 & 0.9 & \textbf{0.0}\\
Natural speech preference & 55 & 786 & 0.44 [0.23,0.65] & 0.9 & 0.4 & 1.0 & \textbf{0.0}\\
Language discrimination and preference & 153 & 2060 & -0.13 [-0.26,0] & 2.3 & 2.0 & 2.9 & \textbf{0.0}\\


\hline
\end{tabular}
\caption{\label{demo-table}This table summarizes the number of effect sizes \(ES\) and the number of participants included in each meta-analysis dataset. The ES estimates represent the aggregated effect sizes and their 95\% confidence intervals from each dataset. The last four columns include the values of $\Delta_{i}$ of corrected Akaike Information Criterion \(AICc\) for the age model with different functional forms: Constant, Linear, Logarithmic, and Quadratic. The values were calculated from subtracting the minimum AICc from the AICc of each model. They were rounded to one decimal. The bold values represent the smallest values among the four functional forms before rounding. Asterisks mark the models that are significantly better than the other models in the same research area.}
\end{table*}




```{r}



age_df_wide <- age_df %>% 
  filter(ic == "AICc") %>% 
  rename(Dataset = dataset) %>% 
  select(REML, model_spec_clean, Dataset) %>% 
  pivot_wider(names_from = model_spec_clean, 
              values_from = REML)
age_df_wide$min <- apply(age_df_wide[c('Linear','Log', 'Quadratic', 'Const')], 1, min)


model_comparison_df <- age_df_wide %>% 
  mutate(
    lin_diff = Linear - min, 
    log_diff = Log - min, 
    qua_diff = Quadratic - min, 
    const_diff = Const - min
  ) %>% 
  select(Dataset, contains("diff")) %>% 
  pivot_longer(cols = contains("diff"), 
               names_to = "diff_type", 
               values_to = "diff_value") %>% 
  mutate(diff_category = case_when(
    diff_value < 4 ~ "no_diff", 
    diff_value >= 4 & diff_value < 7 ~ "low_win", 
    diff_value > 10 ~ "refute_high"
  )) %>% 
  #filter(diff_category != "no_diff") %>% 
  select(-diff_category) %>% 
  pivot_wider(names_from = diff_type, values_from = diff_value) %>% 
  mutate_if(is.numeric, round, 2)
```



Our first research question was about the functional form of the developmental trajectories we observed. We examined four specific forms: constant, linear, logarithmic, and, quadratic, each considered as an age-related fixed effect.  We evaluated the models based on AICc (Table 1).


When using AICc in model selection, the value needs to be contextualized in relation to the lowest AICc among the set of models being compared. Under the conventional interpretation, $\Delta_{i}$ ($AIC_i - AIC_{min}$, where $AIC_i$ is the model being evaluated, and $AIC_{min}$ is the lowest AIC among the set of models) less than 4 suggests minimal evidence against the model with higher AICc; $\Delta_{i}$ above 4 suggests substantial support for the model with lower AICc [@burnham2004multimodel]. With this interpretation framework, the functional forms in most domains can not be meaningfully distinguished, with exceptions in 6 domains. In Mutual Exclusivity, there is a strong preference for the logarithmic model ($\Delta_{Linear}$ = `r filter(model_comparison_df, Dataset == "Mutual exclusivity")$lin_diff` ; $\Delta_{Quad.}$ = `r filter(model_comparison_df, Dataset == "Mutual exclusivity")$qua_diff`; $\Delta_{Const.}$ = `r filter(model_comparison_df, Dataset == "Mutual exclusivity")$const_diff`). We also found a strong preference for the quadratic model in Mispronunciation sensitivity ($\Delta_{Linear}$ = `r filter(model_comparison_df, Dataset == "Mispronunciation sensitivity")$lin_diff`; $\Delta_{Log}$ = `r filter(model_comparison_df, Dataset =="Mispronunciation sensitivity")$log_diff`; $\Delta_{Const.}$ = `r filter(model_comparison_df, Dataset == "Mispronunciation sensitivity")$const_diff`) and a strong preference for the constant model in Simple arithmetic competence ( $\Delta_{Quad.}$ = `r filter(model_comparison_df, Dataset == "Simple arithmetic competences")$qua_diff`;$\Delta_{Linear}$ = `r filter(model_comparison_df, Dataset == "Simple arithmetic competences")$lin_diff`; $\Delta_{Log}$ = `r filter(model_comparison_df, Dataset =="Simple arithmetic competences")$log_diff`). The comparison is less clear-cut in Gaze following, where there is support for the Quadratic model against the Constant model and the Logarithmic model ($\Delta_{Log}$ = `r filter(model_comparison_df, Dataset =="Gaze following (combined)")$log_diff`;$\Delta_{Const.}$ = `r filter(model_comparison_df, Dataset == "Gaze following (combined)")$const_diff`), but the Linear model is comparable with the Quadratic model ($\Delta_{Linear}$ = `r filter(model_comparison_df, Dataset =="Gaze following (combined)")$lin_diff`). Finally, in Statistical sound category learning and Cross situational word learning, we only found evidence against the logarithmic model ($\Delta_{Log}$ = `r filter(model_comparison_df, Dataset == "Statistical sound category learning (habituation)")$log_diff`) and the constant model ($\Delta_{Log}$ = `r filter(model_comparison_df, Dataset == "Cross-situational word learning")$const_diff`), respectively. 


## Methodological Moderators 

In this section, we considered methodological moderators shared by multiple datasets. Given the limited number of studies conducted with neuroimaging methods, we focused our analyses on studies conducted with behavioral methods. Therefore, we excluded studies that were conducted with either fNIRS or EEG. Moreover, to minimize age-related heterogeneity, we only included studies with participants' mean age below 36 months. We added each methodological moderator as an additional fixed effect to the age model with the best-fitting functional form from the previous analysis. All analyses were conducted on the subset of research domains with multiple levels for the moderator of interests. Figure 1 provides a summary of the estimates for moderators.


```{r}
# baseline is looking 
bmeasure_df <- best_fit_bmeasure_model_df %>% 
  filter(grepl("behavioral_measure", term)) %>% 
  mutate(term_print = case_when(
    grepl("other", term) ~ "Behavioral Measure - Other", 
    TRUE ~ "Behavioral Measure - Sucking"
  )) %>% 
  select(dataset, term_print, statistic, p.value, estimate, std.error) 

bmeasure_int_df <- best_fit_bmeasure_interaction_df %>% 
   filter(grepl("behavioral_measure", term) & grepl("age", term)) %>% 
  mutate(term_print = "bmeasure_age_interaction") %>% 
  select(dataset, term_print, statistic, p.value, estimate, std.error) 


# baseline is conditioning 
ep_df <- best_fit_exposure_phase_df %>% 
  filter(grepl("exposure_phase", term)) %>% 
  mutate(term_print = case_when(
    grepl("habituation", term) ~ "Exposure phase - Habituation", 
    grepl("conditioning", term) ~ "Exposure phase - Conditioning"
  )) %>% 
  select(dataset, term_print, statistic,p.value,  estimate, std.error) 

rv_df <- best_fit_real_visual_df %>% 
  filter(grepl("visual", term)) %>% 
  mutate(term_print = "Visual stimulus type - Natural") %>% 
  select(dataset, term_print,statistic, p.value,  estimate, std.error) 


av_df <- best_fit_real_aud_df %>% 
  filter(grepl("aud", term)) %>% 
  mutate(term_print = "Auditory stimulus type - Natural") %>% 
  select(dataset, term_print, statistic, p.value, estimate, std.error) 

ma_df <- best_fit_major_author_df %>% 
  filter(grepl("author", term)) %>% 
  mutate(term_print = "By Major Author") %>% 
  select(dataset, major_author, term_print, statistic, p.value, estimate, std.error)
```


```{r}
all_mod_df <- bind_rows(bmeasure_df, bmeasure_int_df, ep_df, rv_df, av_df, ma_df) %>% 
  mutate(lb = estimate - std.error  * 1.96, 
         ub = estimate + std.error * 1.96) %>% 
  mutate(estimate_print = paste0(round(estimate, 2), " [", round(lb, 2), ", ", round(ub, 2), "]"), 
         z_print = round(statistic, 2), 
         p_print = case_when(
           p.value >= 0.01 ~ paste0("= ", round(p.value, 2)), 
           TRUE ~ "< 0.01"
         )) %>% 
  separate(term_print, into = c("type", "group"), sep = "-")

```

```{r mod_plot, fig.env = "figure*", fig.pos = "t!", fig.width=7, fig.height=5, fig.align = "center", set.cap.width=T, num.cols.cap=2, fig.cap = "Each panel shows the moderator estimates. Each dot represents the estimate of the particular moderator level compared to the baseline. For behavioral measure, the baseline level is looking. Red dots indicate the estimate for studies using sucking measure, and the teal dots indicate the estimates for studies using manual measure. For stimuli exposure method, the baseline level is familiarization. Red, and teal represent the estimates for studies using conditioning and habituation in exposure phase, respectively. For stimuli naturalness, the dots represent the estimate for studies using natural stimuli (e.g. real-world objects; natural speech) compared to studies using artificial stimuli (e.g. pictures, synthetic speech). Error bars show 95\\% confidence intervals."}

plot_moderators(all_mod_df)
```

### Behavioral Measures 

```{r}
bm_models <- all_mod_df %>% filter(type == "Behavioral Measure ")
bm_int <- all_mod_df %>% filter(type == "bmeasure_age_interaction")

int_improved_dataset <- best_fit_bmeasure_model_df %>% 
  distinct(dataset, REML) %>% 
  rename(reg_REML = REML) %>% 
  left_join(best_fit_bmeasure_interaction_df %>% distinct(dataset, REML) %>% rename(int_REML = REML), by = "dataset") %>% 
  filter(int_REML < reg_REML)


```

Meta-analyses have very heterogeneous moderators coded, but many included coding of which behavioral response measure was used in the original study: looking-based behaviors (e.g., looking time or other eye-tracking measures), sucking (as in the high amplitude sucking procedure), and manual behaviors (e.g., pointing, exploration).

In general, nearly all effects were weakly positive such that sucking and manual response modes yielded slightly larger effect sizes, though these effects were not always significant. Behavioral measure was a significant predictor of effect sizes in only two domains, Vowel Discrimination (Native) and Sound Symbolism. In Vowel Discrimination (native), studies with Manual or Sucking behavioral measure has larger effect sizes than studies using looking as the behavioral measure (Manual: $\beta$ = `r filter(bm_models, dataset == "Vowel discrimination (native)", group == " Other")$estimate_print`, *z* = `r filter(bm_models, dataset == "Vowel discrimination (native)", group == " Other")$z_print`, *p* `r filter(bm_models, dataset == "Vowel discrimination (native)", group == " Other")$p_print`; Sucking: $\beta$ = `r filter(bm_models, dataset == "Vowel discrimination (native)", group == " Sucking")$estimate_print`, *z* = `r filter(bm_models, dataset == "Vowel discrimination (native)", group == " Sucking")$z_print`, *p* `r filter(bm_models, dataset == "Vowel discrimination (native)", group == " Sucking")$p_print`). Similarly, in Sound Symbolism, studies with manual behavioral measures also yield larger effect sizes than looking studies ($\beta$ = `r filter(bm_models, dataset == "Sound symbolism")$estimate_print`, *z* = `r filter(bm_models, dataset == "Sound symbolism")$z_print`, *p* `r filter(bm_models, dataset == "Sound symbolism")$p_print`). 

We also explored whether there would be an interaction between the research method and participants’ age. The inclusion of interaction terms did not meaningfully improve the AICc of any of the main model (All $\Delta_{interaction}$ < 2). The current datasets can not distinguish between the interaction effect and the main effect of the behavioral measure. 

### Stimuli Exposure Method

```{r}
ep_models <- all_mod_df %>% filter(type == "Exposure phase ")
```

Stimuli exposure method refers to the type of exposure infants have during the experiments prior to the test events. There are typically three types of stimuli exposure method: 1) an infant would be conditioned to show an orienting behavior (conditioning); 2) an infant would be exposed to a stimulus for a constant amount of time (familiarization); 3) an infant would be presented with some stimuli repeatedly until the magnitude of response drops below a threshold (habituation). We coded these three types of stimuli exposure methods as three levels in the moderator stimuli exposure method.

Stimuli exposure method is a significant predictor of effect sizes in three domains, but the impacts of different stimuli exposure method on effect sizes are mixed. In Vowel discrimination (native), conditioning studies yielded larger effect sizes than familiarization studies ($\beta$ = `r filter(ep_models, dataset =="Vowel discrimination (native)", group == " Conditioning")$estimate_print`, *z* = `r filter(ep_models, dataset =="Vowel discrimination (native)", group == " Conditioning")$z_print`, *p* `r filter(ep_models, dataset =="Vowel discrimination (native)", group == " Conditioning")$p_print`). In Infant directed speech preference, habituation studies produced larger effect sizes than the familiarization studies ($\beta$ = `r filter(ep_models, dataset =="Infant directed speech preference", group == " Habituation")$estimate_print`, *z* = `r filter(ep_models, dataset =="Infant directed speech preference", group == " Habituation")$z_print`, *p* `r filter(ep_models, dataset =="Infant directed speech preference", group == " Habituation")$p_print`), whereas the opposite pattern was found in Language discrimination and preference: habituation studies had smaller effect sizes than the familiarization studies ($\beta$ = `r filter(ep_models, dataset =="Language discrimination and preference", group == " Habituation")$estimate_print`, *z* = `r filter(ep_models, dataset =="Language discrimination and preference", group == " Habituation")$z_print`, *p* `r filter(ep_models, dataset =="Language discrimination and preference", group == " Habituation")$p_print`)

### Stimuli Naturalness 

```{r}
vis_mod <- all_mod_df %>% filter(type == "Visual stimulus type ")
aud_mod <-  all_mod_df %>% filter(type == "Auditory stimulus type ")
```

Next, we considered the effect of stimuli type. We focused on one key dimension: naturalness. For primarily visual stimuli, we considered “natural” to mean stimuli that use real-world objects (e.g. puppets, blocks). We compared these natural stimuli with representation-type stimuli, such as pictures, videos, or drawings. In primarily auditory stimuli, we compared recorded natural speech with synthesized stimuli. 

Natural stimuli has advantages over artificial stimuli across modalities. We found that naturalness was a significant predictor for Label advantage in concept learning, with natural stimuli yielding larger effect sizes than representation-type stimuli ($\beta$ = `r filter(vis_mod, dataset =="Label advantage in concept learning")$estimate_print`, *z* = `r filter(vis_mod, dataset =="Label advantage in concept learning")$z_print`, *p* `r filter(vis_mod, dataset =="Label advantage in concept learning")$p_print`). Similarly, in both Statistical word segmentation and Abstract rule learning, we found a natural speech advantage (Statistical word segmentation: $\beta$ = `r filter(aud_mod, dataset =="Statistical word segmentation")$estimate_print`, *z* = `r filter(aud_mod, dataset =="Statistical word segmentation")$z_print`, *p* `r filter(aud_mod, dataset =="Statistical word segmentation")$p_print`; Abstract rule learning: $\beta$ = `r filter(aud_mod, dataset =="Abstract rule learning")$estimate_print`, *z* = `r filter(aud_mod, dataset =="Abstract rule learning")$z_print`, *p* `r filter(aud_mod, dataset =="Abstract rule learning")$p_print`). 


### Major author 

@margoni2018infants found evidence for an author-based bias in the prosocial agents literature: results produced by certain authors were consistently larger. We evaluated how prevalent this phenomenon was in the literature by coding a “major author” moderator. Authors are considered to be a “major author” if they are listed as authors in more than 15% of the papers in the research area. When multiple major authors co-authored the same set of publications, we considered one author from that author group. When multiple authors were considered as major authors but were associated with different publications, we selected the ones with the most publications in the research area.

```{r}
n_posistive_ef <- nrow(ma_df %>% 
  filter(p.value < .05 & estimate > 0) %>% 
  distinct(dataset))

n_negative_ef <- nrow(ma_df %>% 
  filter(p.value < .05 & estimate < 0) %>% 
  distinct(dataset))
```

We found evidence for a major author effect in `r n_posistive_ef` datasets, where studies produced by the major author were larger than the rest of the papers. In `r n_negative_ef` datasets, however, we also found the opposite patterns, where certain authors produced on average smaller effect sizes than the rest of the literature. 

```{r image, fig.env = "figure", fig.pos = "t!", fig.align='center', fig.width=3, fig.height=3, set.cap.width=T, num.cols.cap=1, fig.cap = "Each dot represents the estimate for studies produced by major author in the particular research area, compared to other studies in the same research area. Error bars show 95\\% confidence intervals. "}

plot_major_author(all_mod_df)
```

## Synthesis 

Finally, we synthesized all 23 datasets by grouping them based on the type of theoretical constructs they represented: Cognitive abilities, Communication, Sounds, and Words. We integrated the predictions from the best-fitting age-based models in Figure 2, showing predictions across the range of measured ages (See SI for each prediction line along with the corresponding data). We found a striking range of functional forms in the developmental trajectories across all types of theoretical constructs. In particular, the magnitudes of some phenomena – online word recognition, gaze following, and mutual exclusivity, for example – increased substantially over development. In contrast, others – sound symbolism, categorization bias, and others – stayed constant at a measurable level without showing developmental increases. We considered several explanations for why some phenomena would be constant: one is that these meta-analyses might correspond to relatively more experience-independent biases. On the other hand, we cannot rule out cross-experiment confounding wherein experimenters test progressively harder stimuli with development, thus counteracting any developmental gains that might otherwise be measured.  


