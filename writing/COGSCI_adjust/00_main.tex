% Template for Cogsci submission with R Markdown

% Stuff changed from original Markdown PLOS Template
\documentclass[10pt, letterpaper]{article}

\usepackage{cogsci}
\usepackage{pslatex}
\usepackage{float}
\usepackage{caption}

% amsmath package, useful for mathematical formulas
\usepackage{amsmath}

% amssymb package, useful for mathematical symbols
\usepackage{amssymb}

% hyperref package, useful for hyperlinks
\usepackage{hyperref}

% graphicx package, useful for including eps and pdf graphics
% include graphics with the command \includegraphics
\usepackage{graphicx}

% Sweave(-like)
\usepackage{fancyvrb}
\DefineVerbatimEnvironment{Sinput}{Verbatim}{fontshape=sl}
\DefineVerbatimEnvironment{Soutput}{Verbatim}{}
\DefineVerbatimEnvironment{Scode}{Verbatim}{fontshape=sl}
\newenvironment{Schunk}{}{}
\DefineVerbatimEnvironment{Code}{Verbatim}{}
\DefineVerbatimEnvironment{CodeInput}{Verbatim}{fontshape=sl}
\DefineVerbatimEnvironment{CodeOutput}{Verbatim}{}
\newenvironment{CodeChunk}{}{}

% cite package, to clean up citations in the main text. Do not remove.
\usepackage{apacite}

% KM added 1/4/18 to allow control of blind submission
\cogscifinalcopy

\usepackage{color}

% Use doublespacing - comment out for single spacing
%\usepackage{setspace}
%\doublespacing


% % Text layout
% \topmargin 0.0cm
% \oddsidemargin 0.5cm
% \evensidemargin 0.5cm
% \textwidth 16cm
% \textheight 21cm

\title{Pokemon go go go}

\usepackage{booktabs}
\usepackage{longtable}
\usepackage{array}
\usepackage{multirow}
\usepackage{wrapfig}
\usepackage{float}
\usepackage{colortbl}
\usepackage{pdflscape}
\usepackage{tabu}
\usepackage{threeparttable}
\usepackage{threeparttablex}
\usepackage[normalem]{ulem}
\usepackage{makecell}
\usepackage{xcolor}

\author{Anjie Cao$^1$  (anjiecao@stanford.edu)
 and \bf{Michael C. Frank$^1$ (mcfrank@stanford.edu)} \\
$^1$Department of Psychology, Stanford University, }

\newlength{\cslhangindent}
\setlength{\cslhangindent}{1.5em}
\newenvironment{CSLReferences}%
  {}%
  {\par}

\begin{document}

\maketitle

\begin{abstract}
HAHA

\textbf{Keywords:}
pikachu; mimikyu; ditto; jigglypufff
\end{abstract}

\hypertarget{introduction}{%
\section{Introduction}\label{introduction}}

In the first three years of life, children undergo a plethora of
developmental changes, transitioning from newborn infants who possess a
limited understanding of language and categories to toddlers who are
able to master a wide range of linguistic and cognitive skills. Despite
a wealth of research examining cognitive development, constructing a
comprehensive theory of cognitive development remains a formidable
challenge. Research in this area generally falls under two categories:
observational research and experimental research. The former provides a
holistic picture of an individual child's development
{[}e.g.@bayley2006bayley{]}, yet it does not provide any concrete
insights into the underlying mechanisms. In contrast, experimental
research allows causal tractions on potential mechanisms, but it tends
to focus on one single construct and does not reveal the connections
between different processes and mechanisms. In this paper, we aim to
provide a quantitative synthesis of experimental work across multiple
areas of developmental psychology, providing insights into the
interrelatedness between psychological constructs. We achieve this goal
by consolidating and integrating 23 meta-analyses of cognitive and
language development compiled on MetaLab, a community-augmented
meta-analysis platform.

Statistical meta-analysis, the technique of aggregating effect sizes
across a systematic sample of experiments, has some unique advantages as
a source of data about developmental processes in early childhood. First
and foremost, it allows researchers to explore questions that are
difficult to address with individual studies. One such example is the
functional form of developmental curves, or how different psychological
processes change over time. Many developmental studies use linear
regression models with age as a predictor, but this assumption of
linearity may not capture the complexities of developmental processes,
especially as they interact with developmental changes in measurement.
For example, some cognitive abilities -- such as relational reasoning --
might follow an inverted-U shape (Carstensen et al., 2019; Walker,
Bridgers, \& Gopnik, 2016), while others -- like early vocabulary size
-- show an exponential increase (Frank, Braginsky, Yurovsky, \&
Marchman, 2021). These non-linear trends can be challenging to identify
and interpret with limited data from individual studies, but
meta-analytic methods can provide a large amount of data across a broad
age range, enabling researchers to evaluate and compare different
functional forms of developmental trajectories.

Meta-analysis can also shed light on the relationships between methods
and theories. Research methods and theories are fundamentally
intertwined, and this is especially true for developmental psychology
{[}@\hspace{0pt}\hspace{0pt}dale2022fundamental{]}. Developmental
theories are often based on interpretations of experimental results,
which are produced by methods that even small changes to the parameters
would substantially change the outcomes. One example is the influence of
familiarization time. It has been proposed that the amount of exposure
infants have prior to the test events can influence infants' direction
of preference (i.e.~novelty preference or familiarity preference)
(Hunter \& Ames, 1988). Although the empirical evidence for this theory
is mixed, this ambiguity has significant downstream consequences on our
understanding of infants' cognitive capabilities (Bergmann \& Cristia,
2016). Debates about infants' arithmetic competencies or their
evaluations of social agents are often centered around the direction of
preferences (Infants arithmetic competencies: Clearfield \& Westfahl,
2006; Wakeley, Rivera, \& Langer, 2000; Wynn, 1993; Evaluation of social
agents: Hamlin, Wynn, \& Bloom, 2007; Salvadori et al., 2015). Due to
the time and resources required for developmental studies, it is often
difficult to directly evaluate the impact of subtle changes in
parameters. Therefore, meta-analytic methods provide a unique
opportunity to investigate the effects of methodological factors on
research findings.

Last but not least, meta-analytic methods make it possible to compare
and connect research findings across research areas. The use of effect
size as the fundamental unit of analysis allows for comparisons across
different domains and research areas. These comparisons can provide
insight into how different processes facilitate learning at different
stages of development and can aid in the development of data-driven
cognitive development theories (Cao \& Lewis, 2022; Lewis et al., 2016).
However, a synthesis across multiple domains requires a database of
multiple meta-analyses. Towards that aim, MetaLab was established to
provide an open database of meta-analyses (Bergmann et al., 2018).
Developmental researchers are invited to deposit their meta-analysis
dataset into MetaLab, and they are encouraged to use the datasets for
custom analyses. As of November 2022, Metalab contains X effect sizes
from 30 different meta-analyses. This resource would allow us to
quantitatively synthesize the insights across different research areas
in developmental psychology.

The plan for this paper is as follows. We first describe the datasets
included in the current synthesis, including our selection criteria and
the descriptive statistics associated with our final dataset. We then
turn to model comparison, comparing the fits of age models under
different functional forms. Next, we present methodological moderators
analysis. Four methodological moderators are selected due to their
theoretical relevances: behavioral measure type, exposure phase type,
stimuli type (audio and visual), and major author effect. Finally, we
present a synthesis of the developmental curves across all of the
domains considered. We end the paper by discussing the implications and
limitations of our current work.

\hypertarget{methods}{%
\section{Methods}\label{methods}}

\hypertarget{datasets}{%
\subsection{Datasets}\label{datasets}}

The datasets were retrieved from \texttt{metalabr}, the R package built
from Metalab. As of November 2022, the package includes 30 individual
meta-analysis datasets covering different research domains in language
learning and cognitive development. We removed 5 datasets from the final
analysis, including 2 due to data quality reasons (Word segmentation
(neuro); Phonotactic learning), 2 due to being observational studies
(Pointing and vocabulary (concurrent) and Pointing and vocabulary
(longitudinal)), and 1 due to being theoretically irrelevant (Video
deficit). We modified 2 datasets to reflect a more accurate
representation of the literature and combined two pairs of datasets
because they measure theoretically identical constructs. To minimize the
heterogeneity in our datasets, we also excluded effect sizes calculated
from participants with clinical diagnoses.

The final dataset contains 23 meta-analysis datasets, including X effect
sizes and X participants. The final dataset and analysis scripts are
available at X.

\hypertarget{analytic-methods}{%
\subsection{Analytic Methods}\label{analytic-methods}}

All analyses reported in this paper are conducted in R using the
\texttt{metafor} package (Viechtbauer, 2010). We specified multi-level
random effect models with random effect structure that includes grouping
by paper and by participant group. We removed the clustering if either
grouping information was missing from the dataset. All moderators are
included as additional fixed-effect in the models. Unless otherwise
specified, all model comparisons are based on the corrected Akaike
Information Criterion (AICc).

\hypertarget{results}{%
\section{Results}\label{results}}

\hypertarget{functional-form-of-developmental-curves}{%
\subsection{Functional form of developmental
curves}\label{functional-form-of-developmental-curves}}

Four functional forms of developmental curves were considered: linear,
logarithmic, quadratic, and constant. For each of the functional forms
except constant, we include mean age in months with the corresponding
form as a fixed effect. Then we evaluated the models based on the
corrected AICc. N out of the 23 datasets has the constant model as the
best-fitting model. X and Y datasets' best-fitting models are the
Quadratic model and logarithmic model, respectively. Finally, among all
datasets considered, Online Word Cognition is the only domain in which
the linear age model provides the best fit.

Model evaluation based on Bayesian Information Criterion (BIC) yielded
similar results with little discrepancy. Under BIC-based evaluation, X
domains' best-fitting models became constant models from higher-order
models (From Quadratic: N = 3; From Logarithmic: N = 2). One domain,
Statistical sound category learning (habituation), has the best fitting
model changed from Constant into Quadratic (STATS).

Table 1 provides a summary of the AICc for each domain's four functional
forms.

\begin{CodeChunk}

\begin{tabular}{l|l|l|l|l}
\hline
Dataset & Linear & Log & Quadratic & Constant\\
\hline
Label advantage in concept learning & 169.48 & \textbackslash{}textbf\{168.53\} & 170.16 & 170.89\\
\hline
Vowel discrimination (native) & 256.49 & 256.13 & 256.78 & \textbackslash{}textbf\{255.15\}\\
\hline
Vowel discrimination (non-native) & 73.25 & 73.36 & 73.15 & \textbackslash{}textbf\{71.69\}\\
\hline
Statistical word segmentation & 128.84 & 129.01 & 128.62 & \textbackslash{}textbf\{127.5\}\\
\hline
Online word recognition & \textbackslash{}textbf\{46.5\} & 46.73 & 46.65 & 48.72\\
\hline
Mutual exclusivity & 421.6 & \textbackslash{}textbf\{415.85\} & 432.76 & 453.07\\
\hline
Sound symbolism & 58.2 & \textbackslash{}textbf\{58.16\} & 58.83 & 61.04\\
\hline
Categorization bias & 300.29 & \textbackslash{}textbf\{299.99\} & 300.37 & 300.9\\
\hline
Familiar word recognition & 27.46 & 28.32 & \textbackslash{}textbf\{27.18\} & 28.86\\
\hline
Abstract rule learning & 140.8 & 141.34 & \textbackslash{}textbf\{140.47\} & 140.91\\
\hline
Switch task & 204.79 & 204.81 & 204.73 & \textbackslash{}textbf\{203.67\}\\
\hline
Mispronunciation sensitivity & 620.05 & 628.16 & \textbackslash{}textbf\{613.67\} & 644.4\\
\hline
Prosocial agents & 82.16 & 81.95 & 82.23 & \textbackslash{}textbf\{80.08\}\\
\hline
Simple arithmetic competences & 22.91 & 23.01 & 22.81 & \textbackslash{}textbf\{16.26\}\\
\hline
Symbolic play & 234.15 & 234.11 & 234.13 & \textbackslash{}textbf\{233.57\}\\
\hline
Natural speech preference & 111.4 & 112.01 & \textbackslash{}textbf\{110.97\} & 111.83\\
\hline
Cross-situational word learning & 79.81 & 81.62 & \textbackslash{}textbf\{79.7\} & 83.71\\
\hline
Language discrimination and preference & 264.7 & 265.59 & \textbackslash{}textbf\{262.65\} & 264.95\\
\hline
Syntactic bootstrapping & 107.28 & \textbackslash{}textbf\{106.99\} & 107.57 & 107.47\\
\hline
Statistical sound category learning (habituation) & 33.47 & 34.54 & 32.94 & \textbackslash{}textbf\{30.46\}\\
\hline
Gaze following (combined) & 151.53 & 159.88 & \textbackslash{}textbf\{149.47\} & 193.2\\
\hline
Word Segmentation (combined) & 328.83 & 328.6 & 329.16 & \textbackslash{}textbf\{327.55\}\\
\hline
Infant directed speech preference & 70.17 & 70.87 & 70.06 & \textbackslash{}textbf\{69.13\}\\
\hline
\end{tabular}

\end{CodeChunk}

\hypertarget{methodological-moderators}{%
\subsection{Methodological Moderators}\label{methodological-moderators}}

In this section, we considered methodological moderators shared by
multiple datasets. Given the limited number of studies conducted with
neuroimaging methods, we focused our analyses on studies conducted with
behavioral methods. Therefore, we excluded studies that were conducted
with either fNIRS or EEG. Moreover, to minimize irrelevant
heterogeneity, we only included studies with participants' mean age
below 36 months. All analyses were conducted on the subset of research
domains with multiple levels for the moderator of interests.

Figure 1 provides a summary of the estimates for moderators.

\hypertarget{behavioral-measures}{%
\subsubsection{Behavioral Measures}\label{behavioral-measures}}

We created the moderator behavioral measure based on Method, an existing
moderator in the Metalab. Behavioral Measure has three levels, looking,
sucking, and other. The level ``other'' includes studies with dependent
measures such as pointing or manual exploration time.

We added Behavioral Measure as an additional fixed effect to the age
model with the best-fitting functional form from the previous analysis.
Behavioral Measure is a significant predictor of effect sizes in two
domains, Vowel Discrimination (Native) and Sound Symbolism. In Vowel
Discrimination (native), studies with Other or Sucking behavioral
measure has larger effect sizes than studies using looking as the
behavioral measure (Other: STATS; Sucking: STATS). Similarly, in Sound
Symbolism, studies with ``Other'' behavioral measures also yield larger
effect sizes than looking studies (STATS).

\hypertarget{exposure-phase}{%
\subsubsection{Exposure Phase}\label{exposure-phase}}

Exposure phase refers to the type of exposure infants have during the
experiments prior to the test events. There are four levels in this
moderator: Conditioning, Familiarization, Habituation, and Test Only. If
a study does not include pre-test training for infants, it is coded as
``Test Only''. Exposure phase is a significant predictor of effect sizes
in four domains. In Language Discrimination and Preference, habituation
studies yield smaller effect sizes than conditioning studies (STATS). A
similar trend is found in Vowel discrimination (native), where both
habituation studies and familiarization studies have smaller effect
sizes than conditioning studies (Habituation: STATS; Familiarization:
STATS).

Interestingly, the results of the comparison between familiarization
studies and conditioning studies are mixed. In the Vowel discrimination
(non-native) dataset, both familiarization and test-only studies produce
smaller effect sizes than the conditioning studies (Familiarization:
STATS; Test-only: STATS.). However, the opposite trend is found in
Natural Speech Preference, with familiarization studies producing larger
effect sizes than the conditioning studies (STATS).

\hypertarget{stimuli-naturalness}{%
\subsubsection{Stimuli Naturalness}\label{stimuli-naturalness}}

Next, we considered the effect of stimuli type. We focused on one key
dimension: naturalness. Since naturalness could be represented
differently for primarily visual stimuli versus primarily auditory
stimuli, we did not collapse across domains.

For primarily visual stimuli, we considered ``naturalness'' to mean
stimuli that use real-world objects (e.g.~puppets, blocks). We compared
these natural stimuli with representation-type stimuli, such as
pictures, videos, or drawings. We found that naturalness is a
significant predictor for Label advantage in concept learning, with
representation-type stimuli yielding smaller effect sizes than natural
stimuli (STATS). In primarily auditory stimuli, we compared natural
speech with synthesized stimuli. In both Statistical word segmentation
and Abstract rule learning, we found a natural speech advantage
(Statistical word segmentation: STAT; Abstract rule learning: STAT).

\hypertarget{major-author}{%
\subsubsection{Major author}\label{major-author}}

Margoni \& Surian (2018) found evidence for authors-based bias in
Prosocial Agents, where results produced by certain authors are
consistently larger than others. We evaluated how prevalent this
phenomenon is in the literature by coding a ``By Major Author''
moderator. Authors are considered ``Major Author'' if they are listed as
authors in more than 15\% of the papers in the literature.

We found evidence for Major Author effect in 7 datasets, where studies
produced by the major author are larger than the rest of the papers. In
3 datasets, however, we also found the opposite patterns, where certain
authors produced on average smaller effect sizes than the rest of the
literature.

\hypertarget{synthesis}{%
\subsection{Synthesis}\label{synthesis}}

Finally, we synthesized 23 datasets by grouping them based on the type
of theoretical constructs: Cognitive, Communication, Sounds, and Words.
We integrated the predictions from the best-fitting age-based models in
Figure 2. We found a striking range of functional forms in the
developmental trajectories across all types of theoretical constructs.

\hypertarget{discussion}{%
\section{Discussion}\label{discussion}}

In this paper, we presented a bird-eye view of developmental psychology
by synthesizing 23 meta-analyses available on MetaLab. We evaluated four
functional forms of the developmental trajectories and we found great
diversity in the shapes of the best-fitting models for each domain. We
also considered the moderating effects of different methodological
factors, including the type of behavioral measure, the type of exposure
phase, stimuli naturalness, and whether the work is done by a ``major
author''. These factors moderate effect sizes from different domains in
heterogeneous ways. Finally, we synthesized the data by integrating the
predictions from all of the best-fitting models based on the type of
theoretical constructs. We found a variety of developmental trajectories
across all types of theoretical constructs.

This current synthesis highlights the variation in developmental
trajectories, challenging the traditional ``milestone'' view of
cognitive development. Under the milestone view, infants would acquire
different cognitive and linguistic skills as they grow (Meylan \&
Bergelson, 2022; Wilks, Gerber, \& Erdie-Lalena, 2010). Our findings
suggest that this view is missing two important details. First, at any
given age, psychological constructs could have a wide range of effect
sizes. For example, at 15 months of age, the predicted effect sizes for
communication skills range from X (DOMAIN NAME) to X (DOMAIN NAME). The
differences between the strengths of the effect may reflect the
differences in how these skills contribute to communication, with some
playing a more significant role than others. In addition, the
development of these skills could follow significantly different
trajectories, with some increasing exponentially with age and others
staying constant throughout early childhood. The heterogeneity of the
developmental process calls for developing a more nuanced and integrated
developmental theory.

The heterogeneity can also partly be attributed to the wide variety of
research methods. In the current analysis, we focused on in-lab
experimental work, and thus the effect sizes may as well reflect how
well the research methods capture the phenomenon of interest. Indeed, we
have shown that subtle experimental procedure changes (e.g.~exposure
phase) could significantly alter the effect sizes. Moreover, methods'
impact varies across domains, with some domains being more susceptible
to methodological factors than others. Therefore, the developmental
trajectories that we document could be influenced by researchers
adapting their methods to participants of different ages. In fact, we
have found that the inclusion of methodological moderators could change
the functional form of the best-fitting models. For example, in Infant
Directed Speech preference, the winning functional form for an age-only
model is constant (STATS? ). However, when the Exposure Phase moderator
is included, the best-fitting model includes age as a linear term
(STATS? ). Our findings call attention to the importance of
understanding methods' nuances: rather than treating methods as a
perfect mirror perfectly reflecting the phenomenon, they should be
regarded as an imperfect lens that could distort our perception of the
phenomenon.

Of course, the same can be said about meta-analysis. Meta-analysis is
not a perfect tool, and can often produce effect sizes significantly
larger than a comparable large-scale replication (Kvarven, Strømland, \&
Johannesson, 2020). Part of the discrepancy can be attributed to the
heterogeneity of research methods that are often minimized in a
large-scale replication (Lewis, Mathur, VanderWeele, \& Frank, 2020).
While we have included methodological moderators in our analysis, it is
highly likely that the coded moderators did not fully reflect the
subtlety of research methods. However, the ``Major author'' effect found
in many research domains could provide a window into understanding the
workings of the research method. In the future, we could compare and
contrast the methods used by ``major authors'' and those by others.
Doing so would allow us to pinpoint the differences and understand which
aspects of the methods really matter, and which do not.

Our ultimate goal is to offer a data-driven synthetic theory of
cognitive development. Here we have made our first step toward that goal
by offering a synthesis of meta-analyses across 23 different research
domains. Moving forward, we aim to expand and refine our synthesis by
including more research areas, correcting potential publication biases,
and accounting for more detailed methodological factors. We would also
like to make more connections between our meta-analysis-based work and
the many ongoing analyses based on large-scale multi-site replication
projects (e.g.~ManyBabies: Frank et al., 2017). Ultimately, we hope our
analysis could provide a solid empirical foundation to help us to better
understand the complex and diverse processes involved in cognitive
development.

\hypertarget{references}{%
\section{References}\label{references}}

\setlength{\parindent}{-0.1in} 
\setlength{\leftskip}{0.125in}

\noindent

\hypertarget{refs}{}
\begin{CSLReferences}{1}{0}
\leavevmode\vadjust pre{\hypertarget{ref-bergmann2016development}{}}%
Bergmann, C., \& Cristia, A. (2016). Development of infants'
segmentation of words from native speech: A meta-analytic approach.
\emph{Developmental Science}, \emph{19}(6), 901--917.

\leavevmode\vadjust pre{\hypertarget{ref-bergmann2018promoting}{}}%
Bergmann, C., Tsuji, S., Piccinini, P. E., Lewis, M. L., Braginsky, M.,
Frank, M. C., \& Cristia, A. (2018). Promoting replicability in
developmental research through meta-analyses: Insights from language
acquisition research. \emph{Child Development}, \emph{89}(6),
1996--2009.

\leavevmode\vadjust pre{\hypertarget{ref-cao2022quantifying}{}}%
Cao, A., \& Lewis, M. (2022). Quantifying the syntactic bootstrapping
effect in verb learning: A meta-analytic synthesis. \emph{Developmental
Science}, \emph{25}(2), e13176.

\leavevmode\vadjust pre{\hypertarget{ref-carstensen2019context}{}}%
Carstensen, A., Zhang, J., Heyman, G. D., Fu, G., Lee, K., \& Walker, C.
M. (2019). Context shapes early diversity in abstract thought.
\emph{Proceedings of the National Academy of Sciences}, \emph{116}(28),
13891--13896.

\leavevmode\vadjust pre{\hypertarget{ref-clearfield2006familiarization}{}}%
Clearfield, M. W., \& Westfahl, S. M.-C. (2006). Familiarization in
infants' perception of addition problems. \emph{Journal of Cognition and
Development}, \emph{7}(1), 27--43.

\leavevmode\vadjust pre{\hypertarget{ref-frank2017collaborative}{}}%
Frank, M. C., Bergelson, E., Bergmann, C., Cristia, A., Floccia, C.,
Gervain, J., et al.others. (2017). A collaborative approach to infant
research: Promoting reproducibility, best practices, and
theory-building. \emph{Infancy}, \emph{22}(4), 421--435.

\leavevmode\vadjust pre{\hypertarget{ref-frank2021variability}{}}%
Frank, M. C., Braginsky, M., Yurovsky, D., \& Marchman, V. A. (2021).
\emph{Variability and consistency in early language learning: The
wordbank project}. MIT Press.

\leavevmode\vadjust pre{\hypertarget{ref-hamlin2007social}{}}%
Hamlin, J. K., Wynn, K., \& Bloom, P. (2007). Social evaluation by
preverbal infants. \emph{Nature}, \emph{450}(7169), 557--559.

\leavevmode\vadjust pre{\hypertarget{ref-hunter1988multifactor}{}}%
Hunter, M. A., \& Ames, E. W. (1988). A multifactor model of infant
preferences for novel and familiar stimuli. \emph{Advances in Infancy
Research}.

\leavevmode\vadjust pre{\hypertarget{ref-kvarven2020comparing}{}}%
Kvarven, A., Strømland, E., \& Johannesson, M. (2020). Comparing
meta-analyses and preregistered multiple-laboratory replication
projects. \emph{Nature Human Behaviour}, \emph{4}(4), 423--434.

\leavevmode\vadjust pre{\hypertarget{ref-lewis2016quantitative}{}}%
Lewis, M., Braginsky, M., Tsuji, S., Bergmann, C., Piccinini, P. E.,
Cristia, A., et al. (2016). A quantitative synthesis of early language
acquisition using meta-analysis.

\leavevmode\vadjust pre{\hypertarget{ref-lewis2020puzzling}{}}%
Lewis, M., Mathur, M., VanderWeele, T., \& Frank, M. C. (2020). The
puzzling relationship between multi-lab replications and meta-analyses
of the rest of the literature.

\leavevmode\vadjust pre{\hypertarget{ref-margoni2018infants}{}}%
Margoni, F., \& Surian, L. (2018). Infants' evaluation of prosocial and
antisocial agents: A meta-analysis. \emph{Developmental Psychology},
\emph{54}(8), 1445.

\leavevmode\vadjust pre{\hypertarget{ref-meylan2022learning}{}}%
Meylan, S. C., \& Bergelson, E. (2022). Learning through processing:
Toward an integrated approach to early word learning. \emph{Annual
Review of Linguistics}, \emph{8}, 77--99.

\leavevmode\vadjust pre{\hypertarget{ref-salvadori2015probing}{}}%
Salvadori, E., Blazsekova, T., Volein, A., Karap, Z., Tatone, D.,
Mascaro, O., \& Csibra, G. (2015). Probing the strength of infants'
preference for helpers over hinderers: Two replication attempts of
hamlin and wynn (2011). \emph{PloS One}, \emph{10}(11), e0140570.

\leavevmode\vadjust pre{\hypertarget{ref-viechtbauer2010conducting}{}}%
Viechtbauer, W. (2010). Conducting meta-analyses in r with the metafor
package. \emph{Journal of Statistical Software}, \emph{36}(3), 1--48.

\leavevmode\vadjust pre{\hypertarget{ref-wakeley2000can}{}}%
Wakeley, A., Rivera, S., \& Langer, J. (2000). Can young infants add and
subtract? \emph{Child Development}, \emph{71}(6), 1525--1534.

\leavevmode\vadjust pre{\hypertarget{ref-walker2016early}{}}%
Walker, C. M., Bridgers, S., \& Gopnik, A. (2016). The early emergence
and puzzling decline of relational reasoning: Effects of knowledge and
search on inferring abstract concepts. \emph{Cognition}, \emph{156},
30--40.

\leavevmode\vadjust pre{\hypertarget{ref-wilks2010developmental}{}}%
Wilks, T., Gerber, R. J., \& Erdie-Lalena, C. (2010). Developmental
milestones: Cognitive development. \emph{Pediatrics in Review},
\emph{31}(9), 364--367.

\leavevmode\vadjust pre{\hypertarget{ref-wynn1993erratum}{}}%
Wynn, K. (1993). Erratum: Addition and subtraction by human infants.
\emph{Nature}, \emph{361}(6410), 374--374.

\end{CSLReferences}

\bibliographystyle{apacite}


\end{document}
